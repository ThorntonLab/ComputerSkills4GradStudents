# Generating graphics to report your research {#graphics}

```{r, echo=FALSE}
# Need to clean up here to avoid the various masking warnings
detach("package:dplyr", unload=T)
```

Graphical representations of concepts and results are a major tool for communicating your science.  This chapter
is about making plots from data, which means that it is mostly about plotting your results. In other words,
 some `pdf` file that you will include in a document for submission or include in a slide.
We are not talking about conceptual diagrams or schematics to illustrate concepts.

Given the topic of this chapter, many aspects are prone to subjective views.  What looks good to one person
does not to another.  These differences of opinion are fine up to a point.  No matter what aesthetic decisions are 
made, a good plot must have several fundamental features:

* It must faithfully represent the underlying data.
* It must be properly labelled.
* It must be detailed appropriately for the target audience.  See \ref{plotting4audiences} below.
* A good faith effort should be made to use color palettes that are friendlier to those with visual impairments.

@Wilke2019-hi is a good overview of making good plots from data.  His book uses R, and specifically `ggplot2` 
plus libraries adding additional features, to make all of the plots. The book is also available
[online](https://serialmentor.com/dataviz/).

## Plotting for different audiences {#plotting4audiences}

As a scientist, you will likely make plots for different audiences:

* Yourself, during the process of exploratory data analysis
* For a lab meeting/meeting with your advisor
* For a report for collaborators/colleagues at other institutions
* For a talk to the public
* For a presentation at a scientific conference
* For a poster
* For publication

The level of technical detail, density of visual elements, font sizes, etc., may differ for each of these target
audiences.  You should expect to remake the same plot multiple times when presenting to different audiences.
When preparing talks and papers, expect to revise figures several times.  The constant tweaking required is a 
good argument to generate your plots with *code* rather than with plotting tools like Excel.  I also advise
against the use of software like Adobe Illustrator and Inkscape for manipulating plots based on data.  These
programs have a mouse-based interface and it is tedious to repeat such operations over and over again while
writing a paper.  (Both tools are fine for conceptual diagrams, though, and is what they are intended for.)

The process of generating figures is iterative, and often requires a lot of experimentation.  If your plotting
code is part of a `git` repo, the `git stash` and `git checkout` commands are helpful to undo any changes since 
your last commit.

## Color schemes for plots

Be thoughtful when choosing your colors.  The defaults for many of the tools discussed here are not great.
There are several online sources for guidance about palettes that are friendly to color-blind viewers. The reality is
that there is no perfect color scheme in the sense that you cannot guarantee that everyone will be able to distinguish
every color.  Rather, the point is to be conscious of this issue and not alternate red and green throughout your paper 
(see @Eisen1998-xj for an example of an influential paper where red-vs-green is the key plotting technique).

When plotting continuous data using any method like a "heat map", the `viridis` palette is a good choice.
It ticks a lot of boxes--perceptually uniform across its range and is at least okay for most of the common
forms of color blindness, which puts it in a leading position in that regard.  The palette is the default
for continuous plots in `matplotlib` as of version 2.0, and it is available as an R package.

In the examples below, I show how to use `viridis` to plot discrete things in order 
to make what we want to focus on stand out more from the rest.

## A tour of plotting using R and Python

You'll learn several things below, mostly by studying the code closely:

* Several data wrangling tips for both R and Python
* You'll see that the main difference between `tidyverse` and `base` R is in the defaults and not necessarily the amount of code.
* Plotting in Python isn't as bad as you've probably been told.

Again, there is no `R` vs Python here.  Use the right tool for the job.  The main point is to generate figures that
belong in publications and that accurately reflect your data and results.

### An example data set 

```{r thornton2019,echo=F, fig.cap="Figure 7a from @Thornton2019-hh. The figure is reproduced here under the terms of the Creative Commons License."} 
knitr::include_graphics("figs/MeanTajdTenLoci.png")
```

We need a data set for plotting.  Instead of using one of the built-in data sets in `R`, we will take a reduced data
set from a recent paper of mine.  The full version of the data set leads to Figure \ref{fig:thornton2019}.  The figure
shows a time series resulting from a simulation of a population adapting to an environmental change.  None of the details
really matter for us, though.  What does matter for us is that the data consists of a large number of values of a
*statistic* called `tajd` in the file.

```{r}
gz = gzfile("data/tajimasd.txt.gz",'r')
data = read.table(gz, header=T)
head(data, 20)
```

The observations of `tajd` are associated with several `conditioning variables`, or `factors`, that may or many 
not be relevant to our analysis:

* `repid` identifies a specific replicate simulation
* `locus` identifies a genetic locus
* `window` identifies a sub-window within a locus
* `generation` identifies a point in time
* $\mu$ and $z_o$ represent parameters of the model.

The specific analysis that we want to plot is the `mean` value of `tajd` for all combinations of $(\mu, z_o)$
for each generation.  Further, for each time point, we want the mean separately for each `window` after
transforming the `window` label into a *distance* from window 5. That window is the one where 
interesting evolution happens, and hence is the "focal" window.  Given that window 5 is our focus,
the plot has to make sure that the data for distance zero "pops" more than for the other windows, which
we accomplish with color and by also making sure that the data for `dist = 0` appears as the *top* line
for each plot.  We will also adjust the time units so that the time of the environmental change is 
the zero point on the `x` axis, which we do by subtracting a constant that you'll see below.  (There 
is no way to just know what that constant is without reading the paper, which you don't need to do, 
because it is just another data wrangling issue as far as you are concerned.)
Finally, we need to adjust the `x` axis limits, as there are more time points
present than we actually need. None of these aesthetic details are going to happen by accident.
Rather, we have to coerce the plotting tools to give us the desired result.

Figure \ref{fig:thornton2019} is based one 256 simulated data sets.  The resulting file is far too big
to store in the GitHub repository for this book.  The file that I have provided is reduced to just the 
first two replicates.  Because of the smaller amount of data, our plots will be noisier than
Figure \ref{fig:thornton2019}, but the plot details are what we are really going for here.

A word of caution before we get started.  Do not expect to understand all the details of the code
shown below right away.  Those of you familiar with these tools may be prone to just glancing over the 
code and thinking that you understand it all.  The reality is that there are lots of subtle details 
shown below.  The code for the final version of the plot took many iterations, and probably many hours,
to get right.  Figures generally take a lot of effort to generate, and preserving them as code is 
very useful.  Later on, when you wonder how to do something, you may remember having done it for a previous 
paper.  You can then dig the code up from GitHub and see how to do it, saving yourself a lot of time.

You will notice that the dimensions of the graphics below differ from plot to plot. In some cases, these
differences are due to different defaults for different tools.  In other cases, I may have manipulated
the output size. None of that really matters, as the final output size is something easily changed in the code.

### R

#### base {#baseplot}

Now, we'll go through the exercise of generating a version of Figure \ref{fig:thornton2019} using
`base` R graphics. @Murrell2005-qn is a standard reference for `base` graphics.  The citation is 
to the first edition, and it has been updated multiple times since I got my copy.

First, we need to turn the `window` column into a `distance` value, representing the distance of a window from
window 5:

```{r}
data$dist <- abs(data$window - 5)
```

Now, we obtain the mean of the statistic for every combination of factors:

```{r}
agg <- aggregate(x=data[c("tajd")], by=data[c("mu","opt","generation","dist")], FUN=mean)
```

Finally, we can shift time such that generation 50,000 is zero:

```{r}
agg$scaled_time <- agg$generation - 10*5e3
```

The result of the above data wrangling is a `data.frame` that we can use for plotting:

```{r}
head(agg)
```

We are going to manually set our `x`-axis limits, so we'll keep them as a global variable:

```{r}
XLIM=c(-2500,4*5e3)
```

To generate the colors, we will take a number of colors from the `viridis` color scheme
equal to the number of lines in each plot.  They will come out darkest to lightest
and we will modify the list so that darker colors are more transparent (smaller
values for `alpha`).

```{r}
library(viridis)
ICOLORS=viridis(length(unique(as.factor(data$dist))))
COLORS=array()
for(i in 1:length(ICOLORS))
{
    ncolor = rgb(as.integer(col2rgb(ICOLORS[i])[1,])/255,
                as.integer(col2rgb(ICOLORS[i])[2,])/255,
                as.integer(col2rgb(ICOLORS[i])[3,])/255,
                alpha=i/length(ICOLORS))
    COLORS[i]=ncolor
}
```

Figure \ref{fig:coplot} shows the output of the following code, generated using the
`coplot` function.  The plot is fine for an exploratory analysis, but it is rather 
far from publication-ready.  The way that the conditioning variables are displayed
takes up far too much space.  We also don't have a legend showing what our lines mean.
The use of the `panel` function to draw the lines themselves is a bit tedious
and we'd prefer that sort of grouping to be more automatic.

```{r coplot, fig.cap="The output from base R's coplot"}
coplot(tajd ~ scaled_time | as.factor(mu)*as.factor(opt), data=agg,
       xlim=XLIM,
       subscripts = TRUE,
       panel=function(x, y, subscripts, ...)
       {
           sset <- agg[subscripts,]
           i <- 1
           # Plot lines is reverse order of variable
           # value such that value 0 is plotted 
           # last/on top.
           for (d in rev(sort(unique(sset$dist))))
           {    
               ssetd = subset(sset, dist == d)
               lines(ssetd$scaled_time, ssetd$tajd, col=COLORS[i])
               i <- i+1
           }
       },
     xlab=c("Generations since optimum shift",expression(mu)),
     ylab=c("Mean Tajima's D", expression(z[o])))
```

It is simpler to make these types of plots with packages where line grouping within
the conditioning variables is automatic, such as `ggplot2` and `lattice`.  Before 
showing how to use them, we will redo our analysis using the `tidyverse` package `dplyr`.

#### Processing the data using the tidyverse

The following code results in a data frame with contents very similar to our
`agg` object above.  One nice feature of `dplyr` is that it is easy to name
the outputs of our analysis.  For example, we name the output of our summary function 
`meand`.  Above, the output of `mean(tajd)` was still named `tajd` in `agg` when using
the `aggregate` function.

We also take the time here to make sure that we have properly ordered `factor` columns in 
our output.  These columns are used to generate the plotting order of the conditioning
variables.

```{r}
library(dplyr)
library(readr)

gz = gzfile("data/tajimasd.txt.gz",'rb')
data = read_delim(gz, "\t") %>%
       mutate(dist = factor(abs(window-5),
                            levels=rev(sort(unique(abs(window-5)))))) %>%
       group_by(generation, dist, mu, opt) %>%
       summarise(meand = mean(tajd)) %>%
       mutate(scaled_time = generation - 10*5e3) %>%
       mutate(fopt = factor(opt, levels=c(1,0.5,0.1)))
head(data)
```

#### ggplot2 {#ggplot2}

@Wilke2019-hi and @Wickham2017-kh are good references on plotting using `ggplot2`.

`ggplot2` is great for quick visualizations of your data:

```{r ggplot2eda, fig.cap="Exploratory data analysis with ggplot2"}
library(ggplot2)
p <- ggplot(data=data,aes(x=scaled_time, y=meand, group=dist)) +
    facet_grid(fopt~mu) +
    geom_line(aes(color=dist,lty=dist)) +
    xlim(XLIM[1], XLIM[2]) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    xlab("Generations since optimum shift") + 
    ylab("Mean Tajima's D")
p
```

This code, shown in Figure \ref{fig:ggplot2eda}, is a good enough exploratory data analysis for most lab meetings. 
The axes are readable and labelled while the plotting factors are semi-labelled.  The colors are not great, so I attempted
to use line styles (`lty`) to make it a bit more readable.  The result isn't great, but at least there is a legend.

Do not let the relative brevity of the above code block hide the fact that some specific manipulations were necessary to
get the desired output:

* The order in which lines are plotted in each panel depends on their sort order as `factors`.
* The ordering of the rows and columns also depends on their sort order as `factors`.

Let's work on getting the figure closer to publication-ready, which will also show us how to improve the labelling.

The following code generates the figure using `ggplot` and the results are shown in \ref{fig:ggplot2}.

```{r ggplot2,fig.cap="The figure generated with ggplot2"}
p <- ggplot(data=data,aes(x=scaled_time, y=meand, group=dist)) +
    facet_grid(fopt~mu,
			   labeller=label_bquote(rows = z[0] == .(fopt),
									  cols = mu == .(mu))) +
    geom_line(aes(color=dist)) +
    xlim(XLIM[1], XLIM[2]) +
    scale_color_manual(values=COLORS) +
    theme_bw() +
    theme(legend.position='top') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    labs(color="Distance from window with causal mutations") +
    xlab("Generations since optimum shift") + 
    ylab("Mean Tajima's D")
p
```

#### lattice {#lattice}

The `lattice` package is the subject of the book by @Sarkar2008-di.  It is based on the `grid`
package extending the `base` functionality (@Murrell2005-qn).  This is the package that 
I used to generate Figure \ref{fig:thornton2019} and the code from the paper is shown below.
It is not much more complex than the `ggplot` versions.  The major subtlety is that you
need to be very careful with custom keys (legends).  It is possible to get the color
order wrong, the label order wrong, or both.

```{r}
library(lattice)

KEY=list(space="top",columns=3,
         title="Distance from window with causal mutations.",
         cex.title=1,
         lines=list(lwd=rep(3,length(COLORS)),col=rev(COLORS)),
         just=0.5,
         text=list(as.character(rev(sort(unique(data$dist))))))

STRIP=strip.custom(strip.names = TRUE,sep=" = ", 
                   var.name = c(expression(mu),expression(z[o])),bg=c("white"))

tajdPlot = xyplot(meand ~ scaled_time| as.factor(mu)*as.factor(opt),
                  group=factor(dist,levels=sort(unique(data$dist))),
                  type='l',data=data,
                  par.settings=simpleTheme(col=COLORS),
                  key=KEY, lwd=3,
                  xlab="Generations since optimum shift",
                  ylab="Mean Tajima's D",
                  xlim=XLIM,
                  scales=list(cex=0.75,alternating=F,x=list(rot=45)),
                  strip=STRIP)
```

The result of this code is shown in Figure \ref{fig:lattice}.

```{r lattice, fig.height=5, fig.width=8, echo=F, fig.cap="The figure generated with the lattice package, dplyr, and viridis"}
tajdPlot
```

### Python

While R is the more popular choice for generating plots, Python has a powerful set of libraries for plotting.
Given that the final result satisfies the criteria of a good plot, the language that you choose is not
especially important.  The only catch is that, if you work in Python and plot in R, then you need a means of getting data between the 
two languages. 

#### Manipulating our data using pandas.

Before we start, we need to read in our data and summarise is as we did in R.
The following code generates a data frame object that we can use for plotting.  The code uses
`pandas` (see Chapter \ref{pandas}) and isn't any more complex than the `dplyr` code above.

```{python}
import pandas as pd
import numpy as np

# Note: the calls to drop are optional.

input_data = pd.read_csv('data/tajimasd.txt.gz', sep='\t', compression='gzip')
input_data.drop(labels=['locus','repid'], axis=1, inplace=True)
input_data['scaled_time'] = input_data['generation'] - 10*5e3
input_data['dist'] = np.abs(input_data['window'] - 5)
data = input_data.groupby(['scaled_time','mu','opt','dist']).mean().reset_index()
data.drop(labels=['generation'], axis=1, inplace=True)
```

#### matplotlib

All plotting in Python is based on [matplotlib](https://matplotlib.org/), which provides an object-oriented
toolkit for static graphics.  Like `base` R, `matplotlib` is a rather low-level solution.  In some respects,
it is simpler than `base` R, but it is more complex in other cases.  I personally like how multi-panel 
layouts are done. Each panel is an instance of the `matplotlib` `Axes` class, meaning that it is a variable
that can be manipulated.

A nice `matplotlib` feature is the ability to use raw \LaTeX\ code in string literals.  It is also possible
to use \LaTeX\ to render the fonts in a plot, but I do not do so here.

```{python}
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
```

First, we define our axis limits for the plot like we did in R.

```{python}
XLIM=(-2500,4*5e3)
YLIM=(-0.4,0.20)
```

Now, we have to construct our plot layout. The example uses `GridSpec` to manage
the panels.  The following code sets up a 3-by-3 list of lists of `Axes` for the plotting.
The three outer lists correspond to the rows of our plot
and the elements within each row are the columns.  We need every panel to have
the same `x` and `y` axis ranges, and we will specify that when creating the panels.
We will define the axis ranges for the upper left panel (row 0, column 0) and tell
the remaining panels that they share its axis limits.

```{python setup}
fig = plt.figure(figsize=(6,6)) # Units are inches

# Generate a 3x4 plot layout
# The 4th column gives us room to make a legend
gs = GridSpec(3, 4, width_ratios=(1,1,1,0.5))

# Create our first panel
row0 = [fig.add_subplot(gs[0,0])]

# Set up the parameters to indicate
# axis sharing
shareargs = {'sharex':row0[0], 'sharey':row0[0]}

# Create the remaining axis objects
row0.extend([fig.add_subplot(gs[0,1], **shareargs),
             fig.add_subplot(gs[0,2], **shareargs)])

row1 = [fig.add_subplot(gs[1,0], **shareargs),
        fig.add_subplot(gs[1,1], **shareargs),
        fig.add_subplot(gs[1,2], **shareargs)]

row2 = [fig.add_subplot(gs[2,0], **shareargs),
        fig.add_subplot(gs[2,1], **shareargs),
        fig.add_subplot(gs[2,2], **shareargs)]

# Hide all the axis ticks and labels 
# for all panels except those along 
# left and bottom edges
for i in row0 + row1:
	i.get_xaxis().set_visible(False)

for i in row0[1:] + row1[1:] + row2[1:]:
	i.get_yaxis().set_visible(False)

# The various x = foo(...) below grab the return
# values of the calls so that they aren't
# printed to the page

# Manually specify where the axis tick marks are,
# the axis font size, and rotation for the x axis.
# This code is over-riding the defaults b/c I like
# the result more.
for i in row2:
	x = i.set_xticks([0, 5000, 1e4, 1.5e4])
	x = i.set_xticklabels([0, 5000, 10000, 15000], rotation=30,
						  fontsize='small')

for i in [row0[0], row1[0], row2[0]]:
	x = i.set_yticks([-0.3, -0.15, 0.0, 0.15])
	x = i.set_yticklabels([-0.3, -0.15, 0.0, 0.15],
						  fontsize='small')
```

Having generated the axis objects, we will group them into a list
of lists and then set the axis limits.  The list of lists
makes the actual plotting steps more convenient.

```{python python_axes}
axes = [row0, row1, row2]
x = axes[0][0].set_xlim(XLIM)
x = axes[0][0].set_ylim(YLIM)
```

When we generate our plot, it will help to be able to map our plotting parameters
back to row and column indexes.  It is straightforward to use a `dict` to build 
these mappings:

```{python}
optindexes = {j:i for i,j in enumerate(reversed(sorted(data.opt.unique())))}
mutindexes = {j:i for i,j in enumerate(sorted(data.mu.unique()))}
```

It is now time to set up our colors. We will pull them from the `viridis` color palette.
In `matplotlib`, the color maps are objects with values that may be retrieved using floats
in the interval `[0, 1]`.  We want good color separation, so we will pull colors uniformly
(on a linear scale) from close to the entire range:

```{python}
ncolors = len(data.dist.unique())
COLORS = [plt.cm.viridis(f) for f in np.linspace(0.01,0.99,ncolors)]
```

The colors are simply tuples of RGB and alpha values:

```{python}
COLORS[:2]
```

The colors go from darkest to lightest and we want alpha transparency value to increase
along the list (*i.e.* brighter colors are more opaque).  Let's make a new color
list where with a gradient of decreasing transparency.  We will also reverse
the list of colors so that it corresponds more naturally to our `dist` variable:

```{python}
COLORS = [(*i[:3], j) for i,j in zip(COLORS, np.linspace(0.2, 1, ncolors))]
COLORS = COLORS[::-1]
```

Using the fourth column in our grid, we use the second row to set up a 
legend:

```{python}
legend_axis = fig.add_subplot(gs[1,3], sharey=axes[0][0])
legend_axis.set_title("Window\ndistance", fontsize='small')
x = legend_axis.axis('off')
```

Having set things up, we can now generate the plot itself, which proceeds via
simple grouping operations using `pandas`.  In the code, the string
literals contain \LaTeX\ code to generate the symbols, which is one of my 
favorite `matplotlib` features.  The front-to-back placement of lines is controlled by the `zorder` parameter.

The code follows and the output is shown in Figure \ref{fig:matplotlib}.

```{python matplotlib, out.height="4in", out.width="4in", fig.align='center',fig.cap="Figure done in Python using matplotlib and pandas"}
import matplotlib.lines as mpl_lines

for n, g in data.groupby(['mu', 'opt']):
    c = mutindexes[n[0]]
    r = optindexes[n[1]]
    for d, dg in g.groupby(['dist']):
        axes[r][c].set_title(r'$\mu = $ {}'.format(n[0]) +', ' +
                             r'$z_o = $ {}'.format(n[1]),
							 fontsize='small')
        axes[r][c].plot(dg.scaled_time, dg.tajd,
						# Annoyance: group name is one
						# variable, so it isn't in a tuple
						color=COLORS[d],
						# Lower zorder = line further to the back
						zorder=len(COLORS)-d)
axes[1][0].set_ylabel("Mean Tajima's D")
axes[2][1].set_xlabel("Generations since optimum shift")

# Now, fill in our legend
lpos = np.linspace(YLIM[0]*0.95,YLIM[1]*0.95, len(COLORS)) 
for c,y in zip(COLORS, lpos):
    legend_axis.add_line(mpl_lines.Line2D((0.1,0.5),
                                           (y+0.0075,y+0.0075),
                                           color=c))

x = legend_axis.set_xlim(0, 1)
for i,y in enumerate(lpos):
    legend_axis.text(0.6, y, i, fontsize='small')
    
plt.show()
```

The major problem with Figure \ref{fig:matplotlib} is the manual legend.  If we need to revise our figures
and put the legend at another location, all of the `GridSpec` bits need to be changed.

#### seaborn

The [seaborn](https://seaborn.pydata.org/) package provides higher-level abstractions for plotting in Python.  The following code
generates Figure \ref{fig:seaborn}.

```{python seaborn, out.height="4in", out.width="4in", fig.align='center', fig.cap="Figure made with the Python seaborn library."}
import seaborn as sns
hue_dict = {i:COLORS[i] for i in data.dist.unique()}
g = sns.FacetGrid(data, row='opt', col='mu', xlim=XLIM,
				  hue='dist', palette=hue_dict,
				  row_order=reversed(sorted(data.opt.unique())),
				  hue_order=len(COLORS) - data.dist.unique() - 1)
g = g.map(sns.lineplot, "scaled_time", "tajd")
x = g.set_axis_labels("Generations since optimum shift", "Mean Tajima's D")
x = g.set_titles(r'$\mu = $' + " {col_name}, " + r'$z_o = $' + " {row_name}")
x = g.add_legend(title="Window distance")
plt.show()
```

The code is very compact, but still required some tricks:

* We are using a custom color setup, so we had to map our `dist` factor to a color via `hue_dict`.
  Note that the final plot doesn't respect the alpha transparency that we set up earlier.  I am
  not sure why, but would consider that an issue.
* The row/column orders are based on standard sorting, so we had to specify the orders. 
  The `row_order` was easy.  We didn't need to mess with `col_order` because the default
  sorting gives the desired result.
* The `hue_order` is non-obvious at first, but it specifies the order in which the lines
  get added to a panel. Our line factor, `dist` starts at 0, but we want that value
  to be plotted last, so that the line is on top.  The ordering is just like how
  `zorder` works (see above). Therefore a simple bit of logic specifies the orders:

```{python}
data.dist.unique()
len(COLORS) - data.dist.unique() - 1
```

The results from `seaborn` are impressive.  The plot is probably publication-quality, or at least very close,
and the amount of code needed is small. The fonts are probably a bit small. The legend placement may not be ideal,
but it is hard-coded in to a `center right` position for this type of plot.  I have read the `seaborn` code and see
why they made this decision, but it is possible that a future release will allow legend placement
"anywhere" like a standard `matplotlib` plot.  The issue is that the layout of these multi-panel plots is a
bit fiddly, which you got a hint of in the previous section.

#### ggplot for Python

There have been a few attempts at a `ggplot` for Python.  The one that seems to be sticking is
[plotnine](https://github.com/has2k1/plotnine).  I have not used it, but am pointing it out in
case it helps someone.

## Generating graphics for publication

For a publication, you must save your figures as files.  You need to consider the file format
for output.  Most people I know are in the habit of saving all figures as `pdf` files.  However,
journals typically want high-resolution `TIFF` files.

I will guess that many of the images shown in this chapter will have trouble being saved to file
as-is.  Font sizes may not work out when you save to a certain size, etc..  In R, it is common that 
what you see in a graphics window and what gets saved to a file differ subtly.  I have experienced this many 
times on systems using the X11 graphics system (Linux and R on older versions of `macOS`).  In other words,
working on your plot by repeatedly saying `source('plot.R')` and looking at what pops out on the screen
may not reflect what you get when you finally save it to a `pdf`.

What follows is a list of loose guidelines:

* Make your graphics files big, with big fonts, and size them down in your document.  In R, I tend to
  make 10 by 10 inch plots with 18 point fonts as a starting point.  I then size them down in \LaTeX\
  via the `\scale` parameter passed to `\includegraphics`.
* Prefer *lossless* image formats!!  From R, this includes `pdf`, `png`, and `tiff`. For the
  last two, set the output resolution to 300 dpi.  For Python, `pdf` and `png` work.
  \LaTeX\ users may also consider `ps` and `eps` (Postscript and Encapsulated Postscript, respectively).
* Prefer formats that your target journal accepts.  This will rule out `png` for sure, and quite often
  `pdf`.  The two Postscript formats are accepted by several journals.
* Prefer file formats that are small enough to include in your documents.  This often rules out 300 dpi
  `tiff`.  For web pages, `png` is ideal, and `pdf` generally not allowed.

### Image Magick

You are now certainly confused, as we have ruled out every format!  That's right, there is no objectively perfect file 
format, bringing us to our final guideline:

* Learn how to reliably convert between formats without losing quality.

[ImageMagick](https://imagemagick.org/index.php) exists to help with this last guideline.  It is a command-line Swiss
Army knife for image manipulation.  Yes, you could use various graphical tools to convert.  Apple's Preview on `macOS` is
great for this, but it cannot be easily automated.

For example, the following `bash` loop will convert all of your `pdf` files into 300 dpi `tiff` while also flattening
all layers and replacing any transparent backgrounds with white:

```{sh, eval=F}
for i in *.pdf
do
    n=`basename $i .pdf`
    convert -density 300 -flatten -background white -compress lzw $i $n.tiff
done
```

Oh, we also compressed the file using the `lzw` algorithm, which is accepted by most journals that I work with.
Loops like this one are hard to beat when you have to convert possibly dozens of graphics files.

`ImageMagick` can also help you with other issues, like if your files are rejected for having the wrong color space
encoding.  In the past, graphics output from R were rejected from PLoS journal's automatic "QC" system for this reason.
To make figures that passed, we set the R scripts to output 600 dpi `tiff` and then run this loop:

```{sh, eval=F}
#!/bin/bash

# Credit to Jaleal Sanjak for working this out for one or more
# of his PhD papers.

for i in $(ls *.tif | grep -v _compressed)
do
    n=`basename $i .tif`
    convert $i -set colorspace RGB -layers flatten -alpha off -compress lzw -depth 8 \
    -density 600 -adaptive-resize 4500x2400  $n"_compressed".tif

    convert $n"_compressed".tif $n"_compressed".pdf
done
```

In hindsight, the 600dpi output may have been overkill, but it worked.  The final conversion to `pdf` was for inclusion in 
our \LaTeX\ document.

On some systems, you may get errors when trying to `convert` from `pdf` (or `ps`/`eps`) to other formats.  This error
is in place because of a security flaw in `ghostscript`.  See [here](https://stackoverflow.com/questions/52861946/imagemagick-not-authorized-to-convert-pdf-to-an-image/52863413#52863413) for how to enable conversion (link is to Stack Overflow).

## Generating graphics for talks

All of the above applies, but you can get away with smaller files like `png`.

Expect to have to modify your figures for talks.  You are all but guaranteeing a bad talk if you take figures directly
from your paper and drag them into Powerpoint.  Papers from publications are often far too detailed, and the fonts too small,
to be visible from the back of the room. Every talk is a job interview, so don't be lazy here.
For the figures shown in this chapter, I would reduce them to a single row or 
perhaps even to a single panel.  The idea that you may need to change your figures **yet again** reinforces the importance
of generating them with code that is under version control.

## Hints for Google Docs

If you write your documents using this tool, then you need to make sure your image files are all on the Google Drive
using the same account as your Docs.  The reason is the Docs only allows you to drag small image files from your local
machine into a document.  A larger file will get rejected, but then will work fine if you select "Insert image from Drive".
Even if you manage to drag in a local file, Docs may automatically downsize it, resulting in a loss of image quality.

Google Docs seems to prefer `png` above all else.  I expect that these guidelines apply to the other Google tools, too.

## Interactive graphics

Academia has long been subjugated by the process of publication via dead trees.  Even with most/all of our journals being
accessed *exclusively* online, that access is still most often based around the display of a `pdf` file with static graphics.
It may sometimes be the case that a graphic can be made more effective by making it interactive.  Several tools exist to 
do this, and they generally result in an HTML output with some embedded JavaScript.  You may want to check out the following tools:

* [shiny](https://shiny.rstudio.com/) for R.
* [plotly](https://plot.ly/r/) for R.
* [bokeh](https://docs.bokeh.org/en/latest/) for Python.
* [plotly](https://plot.ly/python/) for Python.
* [holoviews](http://holoviews.org/) for Python.

All of these tools are designed to run in the appropriate notebook system for each language (see Chapter \ref{notebooks}).
They also allow export to `html` files that you may display online, including through a GitHub site.  [Here](http://www.molpopgen.org/2018/11/18/visualizimg-simulations.html) is an example that I generated using `holoviews` and a Jupyter notebook.
